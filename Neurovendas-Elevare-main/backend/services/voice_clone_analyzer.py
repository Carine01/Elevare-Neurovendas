"""
Voice Clone Analyzer - AnÃ¡lise de Amostras de Voz

Analisa os textos de exemplo fornecidos pelo usuÃ¡rio para extrair
padrÃµes de escrita que serÃ£o usados pela IA para replicar o estilo.
"""

import re
from typing import List, Dict, Tuple
from collections import Counter
import emoji


class VoiceCloneAnalyzer:
    """Analisador de padrÃµes de escrita para clone de voz IA"""
    
    def __init__(self, voice_samples: str):
        """
        Inicializa o analisador com as amostras de texto.
        
        Args:
            voice_samples: Texto com 3-5 exemplos de escrita do usuÃ¡rio
        """
        self.voice_samples = voice_samples.strip()
        self.sentences = self._split_sentences()
        self.paragraphs = self._split_paragraphs()
        self.words = self._extract_words()
    
    def _split_sentences(self) -> List[str]:
        """Divide o texto em frases individuais"""
        # Regex que respeita reticÃªncias, mas separa por . ! ?
        pattern = r'(?<!\.\.)(?<=[.!?])\s+(?=[A-ZÃÃ‰ÃÃ“ÃšÃ‚ÃŠÃ”ÃƒÃ•Ã‡])|(?<=[.!?])\n'
        sentences = re.split(pattern, self.voice_samples)
        return [s.strip() for s in sentences if s.strip()]
    
    def _split_paragraphs(self) -> List[str]:
        """Divide o texto em parÃ¡grafos"""
        paragraphs = re.split(r'\n\s*\n', self.voice_samples)
        return [p.strip() for p in paragraphs if p.strip()]
    
    def _extract_words(self) -> List[str]:
        """Extrai todas as palavras do texto (sem pontuaÃ§Ã£o)"""
        # Remove emojis primeiro
        text_no_emoji = emoji.replace_emoji(self.voice_samples, replace='')
        # Remove pontuaÃ§Ã£o e divide em palavras
        words = re.findall(r'\b[a-zÃ¡Ã©Ã­Ã³ÃºÃ¢ÃªÃ´Ã£ÃµÃ§A-ZÃÃ‰ÃÃ“ÃšÃ‚ÃŠÃ”ÃƒÃ•Ã‡]+\b', text_no_emoji)
        return [w.lower() for w in words]
    
    def calculate_avg_sentence_length(self) -> float:
        """Calcula comprimento mÃ©dio das frases em palavras"""
        if not self.sentences:
            return 0.0
        
        total_words = sum(len(s.split()) for s in self.sentences)
        return round(total_words / len(self.sentences), 1)
    
    def calculate_emoji_frequency(self) -> float:
        """Calcula frequÃªncia de emojis (emojis por 100 caracteres)"""
        emoji_count = len([c for c in self.voice_samples if c in emoji.EMOJI_DATA])
        text_length = len(self.voice_samples)
        
        if text_length == 0:
            return 0.0
        
        return round((emoji_count / text_length) * 100, 2)
    
    def calculate_question_ratio(self) -> float:
        """Calcula proporÃ§Ã£o de frases interrogativas"""
        if not self.sentences:
            return 0.0
        
        questions = sum(1 for s in self.sentences if '?' in s)
        return round(questions / len(self.sentences), 2)
    
    def calculate_exclamation_ratio(self) -> float:
        """Calcula proporÃ§Ã£o de frases exclamativas"""
        if not self.sentences:
            return 0.0
        
        exclamations = sum(1 for s in self.sentences if '!' in s)
        return round(exclamations / len(self.sentences), 2)
    
    def calculate_paragraph_avg_lines(self) -> float:
        """Calcula mÃ©dia de linhas por parÃ¡grafo"""
        if not self.paragraphs:
            return 0.0
        
        total_lines = sum(p.count('\n') + 1 for p in self.paragraphs)
        return round(total_lines / len(self.paragraphs), 1)
    
    def detect_caps_usage(self) -> bool:
        """Detecta se usa CAPS para Ãªnfase"""
        # Procura por palavras inteiras em maiÃºsculas (mÃ­nimo 2 letras)
        caps_pattern = r'\b[A-ZÃÃ‰ÃÃ“ÃšÃ‚ÃŠÃ”ÃƒÃ•Ã‡]{2,}\b'
        caps_matches = re.findall(caps_pattern, self.voice_samples)
        
        # Ignora siglas comuns
        common_acronyms = {'DRA', 'DR', 'USA', 'EUA', 'CNPJ', 'CPF', 'RG'}
        real_caps = [w for w in caps_matches if w not in common_acronyms]
        
        return len(real_caps) > 0
    
    def detect_ellipsis_usage(self) -> bool:
        """Detecta se usa reticÃªncias (...)"""
        return '...' in self.voice_samples or 'â€¦' in self.voice_samples
    
    def extract_common_phrases(self, min_frequency: int = 2) -> List[str]:
        """
        Extrai frases/expressÃµes mais comuns (2-5 palavras).
        
        Args:
            min_frequency: FrequÃªncia mÃ­nima para considerar comum
            
        Returns:
            Lista de frases comuns ordenadas por frequÃªncia
        """
        # Gera n-grams de 2 a 5 palavras
        ngrams = []
        
        for n in range(2, 6):  # 2, 3, 4, 5 palavras
            for i in range(len(self.words) - n + 1):
                ngram = ' '.join(self.words[i:i+n])
                ngrams.append(ngram)
        
        # Conta frequÃªncias
        counter = Counter(ngrams)
        
        # Filtra por frequÃªncia mÃ­nima e pega top 10
        common = [phrase for phrase, count in counter.most_common() if count >= min_frequency]
        
        return common[:10]
    
    def assess_vocabulary_level(self) -> str:
        """
        Avalia nÃ­vel de vocabulÃ¡rio: simples, moderado, complexo.
        
        Baseado em:
        - Tamanho mÃ©dio das palavras
        - Diversidade lexical (type-token ratio)
        """
        if not self.words:
            return "simples"
        
        # Tamanho mÃ©dio das palavras
        avg_word_length = sum(len(w) for w in self.words) / len(self.words)
        
        # Diversidade lexical (palavras Ãºnicas / total de palavras)
        unique_words = len(set(self.words))
        lexical_diversity = unique_words / len(self.words)
        
        # ClassificaÃ§Ã£o
        if avg_word_length > 6 and lexical_diversity > 0.6:
            return "complexo"
        elif avg_word_length > 5 or lexical_diversity > 0.5:
            return "moderado"
        else:
            return "simples"
    
    def detect_formality(self) -> str:
        """
        Detecta nÃ­vel de formalidade do texto automaticamente.
        
        Returns:
            'muito_informal', 'informal', 'equilibrada', 'formal', 'muito_formal'
        """
        # Indicadores de informalidade
        informal_markers = [
            r'\bÃ³\b', r'\buÃ©\b', r'\bnÃ©\b', r'\btipo\b', r'\btÃ¡\b',
            r'\bvc\b', r'\bvcs\b', r'\bpq\b', r'\btbm\b',
            'rsrs', 'kkk', 'haha',
        ]
        
        # Indicadores de formalidade
        formal_markers = [
            r'\bpor favor\b', r'\bsolicito\b', r'\bcorresponder\b',
            r'\batenciosamente\b', r'\bcordialmente\b',
            r'\bconsidera-se\b', r'\bobserva-se\b', r'\bconclui-se\b',
        ]
        
        informal_count = sum(
            len(re.findall(marker, self.voice_samples.lower()))
            for marker in informal_markers
        )
        
        formal_count = sum(
            len(re.findall(marker, self.voice_samples.lower()))
            for marker in formal_markers
        )
        
        # Verifica uso de emojis (indicador de informalidade)
        has_emojis = self.calculate_emoji_frequency() > 1.0
        
        # Verifica contraÃ§Ãµes (tÃ¡, pra, nÃ© - indicador de informalidade)
        contraction_pattern = r"\b(tÃ¡|nÃ©|pra|pro|tÃ´|cÃª)\b"
        has_contractions = bool(re.search(contraction_pattern, self.voice_samples.lower()))
        
        # LÃ³gica de decisÃ£o
        if informal_count >= 3 or (has_emojis and has_contractions):
            return "muito_informal"
        elif informal_count >= 1 or has_emojis or has_contractions:
            return "informal"
        elif formal_count >= 3:
            return "muito_formal"
        elif formal_count >= 1:
            return "formal"
        else:
            return "equilibrada"
    
    def analyze(self) -> Dict:
        """
        Executa anÃ¡lise completa e retorna todos os resultados.
        
        Returns:
            Dict com todas as mÃ©tricas de anÃ¡lise
        """
        return {
            "avg_sentence_length": self.calculate_avg_sentence_length(),
            "emoji_frequency": self.calculate_emoji_frequency(),
            "question_ratio": self.calculate_question_ratio(),
            "exclamation_ratio": self.calculate_exclamation_ratio(),
            "paragraph_avg_lines": self.calculate_paragraph_avg_lines(),
            "uses_caps": self.detect_caps_usage(),
            "uses_ellipsis": self.detect_ellipsis_usage(),
            "common_phrases": self.extract_common_phrases(),
            "vocabulary_level": self.assess_vocabulary_level(),
            "formality_detected": self.detect_formality(),
        }


def analyze_voice_samples(voice_samples: str) -> Dict:
    """
    FunÃ§Ã£o helper para anÃ¡lise rÃ¡pida de amostras de voz.
    
    Args:
        voice_samples: Texto com exemplos de escrita
        
    Returns:
        Dict com mÃ©tricas de anÃ¡lise
        
    Example:
        >>> samples = "Olha, vou ser sincera... HarmonizaÃ§Ã£o nÃ£o Ã© milagre! ðŸ’Ž"
        >>> result = analyze_voice_samples(samples)
        >>> print(result['formality_detected'])
        'informal'
    """
    analyzer = VoiceCloneAnalyzer(voice_samples)
    return analyzer.analyze()
